
\documentclass{article}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}


\usepackage{amsmath}

\title{Week 4 Report: Backpropagation review}
\date{2017-04-27}
\author{Lam Dang}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\pagenumbering{arabic}
\section{Gradient Descent}
Gradient Descent is a popular method in Data Mining to obtain the most suitable model. Gradient Descent usually used in tandem with other algorithm in order to create a model for prediction or classification, such as Linear Regression, Neural Network etc. \par
The concept of Gradient Descent is to "go down" the gradient "well" at a predefined rate. After each "step" the algorithm re-evaluate for new weight vector. These steps are repeated until a pre-defined number of iterations or until the global minima have been reach.\par
Another view on the algorithm is to consider it as an optimization process \cite{lecun_theoretical_1992}. In this view, Gradient Descent is considered as a constrained minimization problem.
\subsection{Mathematical Model}
Bishop \cite[p240]{bishop_pattern_2006} provide a simple formular to update weights vector base on Gradient information
\begin{equation}
w^{(\tau+1)} = w^{(\tau)} - \eta \nabla E (w^{(\tau)})
\end{equation}
whereas $\eta$ is the pre defined learning rate and $E (w^{(\tau)})$ is the loss function defined in the following section.
\subsection{Loss Function}
Bishop \cite[p242]{bishop_pattern_2006} also describe an evaluation of weight vector base on derivative of error function. 
\begin{equation}
E(w) = \frac{1}{2} \sum_{n=1}^N || y(x_n,w)-t_n||^2
\end{equation}
\pagebreak

\section{Backpropagation}
Backpropagation is one of the the dominating method for learning with Artificial Neural Network. The concept of Backpropagation is relatively simple however they can provide robust learning scheme and models. \cite{lecun_theoretical_1992}
\subsection{Algorithm}
Backpropagation algorithm use indefinitely differentiable activation function. We denote these function as $F$ and their derivative as $F'$. The algorithm went through 2 steps
\subsubsection{Forward Propagation}
The input $X_i$ is feed into the network. The functions $F(X_i)$ are calculated and propagate forward into next layers. The derivative functions $F'(X_i)$ are stored
\subsubsection{Back Propagation}
The 
\nocite{rojas_neural_1996}
\subsection{Weight update}
\subsection{Learning Rate}
\pagebreak

\section{Current Result}
\subsection{Linear Regression}
\subsection{Extreme Learning Machine}
\subsection{Backpropagation}

\bibliography{Week4}
\bibliographystyle{unsrt}
\end{document}